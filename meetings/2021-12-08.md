---
parent: Meetings
title: 2021-12-08
---

# ASWF CI Working Group

Meeting:   08 December 2021

[https://zoom.us/j/757849640?pwd=QzE1K2hrL2FHSFhKK3h5Z3BWTFJsZz09](https://zoom.us/j/757849640?pwd=QzE1K2hrL2FHSFhKK3h5Z3BWTFJsZz09)

## Attendees

* Jean-Francois Panisset (VES Technology Committee)
* Aloys Baillet (NVIDIA)
* Larry Gritz (Sony Imageworks)
* Sergio Rojas (Arena World)
* Andrew Grimberg (LF RelEng)
* Ryan Botriell (spk / ILM)
* Jeff Bradley (DreamWorks)
* Scott Wilson (Rust WG)
* Jean-Christophe Morin (OTIO)
* Christina Tempelaar-Lietz (Epic Games)
* Tiago Carvalho (Rust WG)

## Apologies

## New items

* [OTIO Python Wheels infrastructure](https://gist.github.com/JeanChristopheMorinPerso/f5cd307243a8164b1da9ebbf63152c93): Jean-Christophe Morin from OTIO

    * Went through basic of work, most of you should be familiar on building Python components

    * 22 wheels are produced via GitHub actions, pushed to PyPI, Python 2.7, 3.7, 3.8, 3.9 (except 3.9.0 due to pybind11 issues), Linux, macOS, Windows

    * Seems to work, mostly Windows where OTIO had most issues in the past, one GH issue per week related to using OTIO on Windows

    * Built on project cibuildwheel, also used in OCIO, takes care of building wheels for you. On Linux uses the minilinux Docker images, which are Docker images used to test that Wheels work on most Linux platforms.

    * On Windows it downloads Python from a source, installs it into a temp directory, and that's the Python used for building (same on macOS)

    * OTIO uses this to build C / C++ extensions, added a step to build the steps in the GHA pipelines, most of the hard work was Windows with Python 2.7, since it is expected to be compiled with old VS compiler (2008), which is no longer generally available. Building with VS 2019 and get extensions to work with VS 2008 compiled Python was a bit tricky.

    * Amount of changes required for cross platform wheels wasn't too bad

    * Larry: can you elaborate on issue with pybind11 and Python 3.9.0? Bug got fixed in Python 3.9.1, but also got fixed on pybind11, but to avoid problems for sure, excluded Python 3.9.0 to avoid the problem, setup.py excludes Python 3.9.0. Larry: is it on all platforms? JC: [https://github.com/python/cpython/pull/22670](https://github.com/python/cpython/pull/22670)

    * JC: to get multi-platform builds going in a project, keep it simple as much as possible and go to the basics. If something isn't working, don't add hacks / workarounds since it can get ugly with Python. Finding the Python interpreter itself can be tricker. In OTIO, put a minimum on CMake (3.18?) which should work everywhere as much as possible. Don't have to patch the FindPython CMake macro, so bumped the CMake version. Didn't want to add too much complexity to the project. Larry: foundational packages can end up driving the required version of CMake. JF: last meeting, we came up with:

        * 2019: cmake-3.12.4

        * 2020: cmake-3.18.4

        * 2021: cmake-3.19.3

        * 2022: cmake-3.20.5

    * JF: CMake may be installed by the Python toolchain when they pip install otio

    * Christina: how does cibuildwheel fit in with aswf-docker images, or is it not used for actual CI test runs? JC: use minilinux docker images by default, but can use custom Docker images, so could use the aswf-docker images if you want. Went with minilinux images since they provide what OTIO needs, helps to ensure the builds will work everywhere. Could have separate Python wheels compiled against ASWF docker containers. A lot of scientific Python images are built using minilinux images, and work fine in Maya / Houdini. Christina: do you only run cibuildwheel when generating the new matrix of packages, or also when running test? JC: run all the time for every commit, don't give it a lot of thought, at least we know that every PR can build the wheels correctly, in the future, want to first generate the wheels, than run the tests against the wheels. Technically it's implicit, when we run the tests, we pip install, and pip will build and install the wheel. But that doesn't guarantee that it will be built the same.

    * Aloys: minilinux looks really interesting, but based on CentOS 6, so oldest glibc version available. Some of our other projects (especially built on CentOS 7) probably won't install, especially external packages we just yum install. JC: building on a really old CentOS means that OTIO can be installed in a lot of places, OTIO may be in a different situation of other projects, in the editing world, may not know where the code is going to run (Raspberry PIs…). So compiling against CentOS 6 gives us the widest coverage.

    * Aswf-docker builds OTIO, but doesn't build wheels per say. Once every package has a good packaging solution, ASWF-specific build is less relevant. No way to do this for pure C++ packages.

    * JC: was lucky that it was a pure Python project originally, but starting to have more and more C++ for performance. As to having ASWF-compliant wheel, not sure how it would work / integrate in Python ecosystem. Pip is the only Python installer, it knows about tags, wheel filename includes tags, wouldn't necessarily have a specific one for ASWF, would need to teach the installer how to install the wheel. Not sure how we could provide the wheels to the ecosystem. Could leverage LF Artifactory, can either proxy artifacts, or host them. But may not be useful. Aloys: everyone wants the wheels in PyPI, no point in having them anywhere else. Packaging OTIO in Conan makes sense once the C++ part is separate from the Python part. But as long as OTIO doesn't have upstream dependencies, no reason to put it anywhere else. Nice to not have upstream dependencies and be able to build on older environment.

    * JC: not entirely done yet, need to run tests against built wheels, build and test against macOS M1, so not quite done yet. Available on Slack, don't hesitate to reach out.

* Rust CI needs: Tiago Carvalho from Rust WG

    * Tiago: Rust WG is working on creating bindings for Rust for the major AWSF libraries, starting with OpenEXR, bindings are in a good place. Before moving forward, pushing for CI process to enable everyone to contribute.

    * Not a perfect situation: bindings are based on a tool developed by Rust WG called cppmm which uses LLVM and Clang to perform "magic" to parse binding file and generate the required crates that generates the Rust bindings into the C++ code.

    * Cppmm is the focus of the CI issues, we need something that brings cppmm for CI builds, LLVM 11 is also a heavy dependency.

    * Have been looking at aswf-docker containers, but since they are based on CentOS, at the time not up to LLVM 11, and some linking issues found by Scott. Linking issues might be fixable in newer version of Rust, so may want to try that. But some other questions to understand if aswf-docker containers would be possible to use.

    * For OpenEXR crate, decided that major and minor version will match OpenEXR library version, but patch version will depend on both the patch version of the OpenEXR library and the cppmm version we are running. So if we bump the cppmm version, we also bump the patch version. Could be an issue given how VFX Reference Platform locks down version numbers.

    * Scott: we have a Docker container within OpenEXR binding repo, responsible for a build environment, tested CentOS earlier but ran into link errors, solved in a nightly version of Rust. Rust has stable / beta / nightly versions, trying to target the stable releases, so having to rely on a nightly version is less desirable. May have been addressed in latest stable version of Rust, will try adding the required link time flag to the build. But on Ubuntu or other more recent distribution, don't have to worry about that.

    * Aloys: latest aswf-docker containers is up to LLVM 13. Larry: what's the minimum LLVM? It's not a minimum, has to be LLVM 11 explicitly dues to the way cppmm works, it using LLVM AST system to parse the C++ code, so if you switch to a newer version of LLVM, that could break cppmm. Tiago: need specifically llvm11 to build cppmm. JF: is the LLVM 11 requirement runtime, or build time only? Scott: haven't tested it, seems like it could work. In theory could build have cppmm create its own build artifacts, and have other containers pull down the artifacts. So could build in a specific LLVM 11 ASFW Docker and create a Conan package.

    * Scott: there are two LLVM requirements: cppmm and Rust itself. Larry: does using Rust constrain the version of LLVM / Clang used by the rest of a project? Scott: with the Rust compiler itself, not sure if the LLVM compile is static, but it does appear to be self contained, you don't need to install LLVM before you install Rust. Larry: concerned about cases where you want a specific LLVM version for other reasons, and want to make sure that won't confuse the Rust compiler. Scott: haven't tested it, but haven't seen issues with having other versions of LLVM installed on a machine. Rust does seem to be self contained. Larry: seems reasonable about Rust, just want to make sure cppmm is properly insulated, to avoid situations where you have to choose between generating bindings with cppmm and using the LLVM version you want.

    * Other option on the CI server is to not worry about cppmm at all, just have the automatically generated code already in place. Trying to figure out the best way to do it.

    * Tiago: in theory, could generate bindings on Ubuntu (say), that crate will then be usable everywhere (Ubuntu, CentOS, macOS, Windows…). So binding generation could be in dedicated container that doesn't impact anything else. Crate isn't a binary artifact, it contains Rust / AST format.

    * Scott: it's like an "sdist" in Python

    * Tiago: will copy the source code of OpenEXR and bundle it inside the crate, so people pulling down the crate will build it themselves. Not focussed too much on making use of ASWF containers, if we have lightweight container on Ubuntu, will solve our issues. Also not bound to version specific by VFX Platform, since we are a new project, need some agility in the version.

    * Tiago: even if it doesn't make sense to use aswf-docker containers, will benefit with having the WG helping to create and automate generation of the Ubuntu-based builds to generate the bindings. Maybe cppmm on Conan could simplify some things, as a way to embrace the aswf-docker containers.

    * Working on moving repos from vfx-rust GitHub org to ASWF GitHub org. Also want to automate CI publish pipeline.

* Initial Windows Docker Support

    * Using Windows Docker images maintained by Microsoft, found bug when compiling files where PDB generator inside VS 2017 doesn't work inside Hyper-V if the volume is mounted on the host, which has to be done for Conan builds.

    * OK to start at VFX Platform CY2022 due to [VS 2017 / CMake / PDB / Windows container build issues](https://gitlab.kitware.com/cmake/cmake/-/issues/17566) But would be nice to have older versions. VM approach may be faster on GHA. Will need a local VM and make it look like GHA runner, do they provider a recipe? JC: GitHub provides the [list of packages](https://github.com/actions/virtual-environments) installed on a runner, you can technically do it. It's all PowerShell scripts, reasonably clean. Aloys: will try to get something like that running. Would be good to have a reproducible build environment. There is a Docker recipe, can build a few packages which are fairly useless (ninja, cmake, but you can download installers for them), But there will be some value once we start hitting tbb, boost… (tricky to install multiple versions on Windows). Will be interested to hear of main pain points on other OSes. Trying to make it look like the Linux version, don't want to build Python from source since that's a pain on Windows, so just installing in a folder and getting Conan to generate package from that folder. There's value in having Conan packages that repackage existing packages. Still very fresh, thinking about approach, comments are welcome.

    * Windows VM for CI experimentation? macOS (reuse MacStadium server?)

* GSoC / Summer Internship

* CI external resources survey

## Tools

* [Exodus](https://github.com/intoli/exodus) : a tool to extract binaries and DSOs from build environments to support older distributions

* [Release Please](https://github.com/googleapis/release-please) from Google: automates CHANGELOG generation, the creation of GitHub releases, and version bumps for your projects. It does so by parsing your git history, looking for [Conventional Commit messages](https://www.conventionalcommits.org/), and creating release PRs

* [Native GitHub Release Notes generation](https://docs.github.com/en/repositories/releasing-projects-on-github/automatically-generated-release-notes)

## Follow Ups

* Distro maintainer feedback from TAC meeting

    * [Recording of presentation by Richard Shaw from Fedora to TAC](https://drive.google.com/file/d/1CQp1jRn4mVWawsPJEDHT7kAST_cwu-BA/view?usp=sharing)

* Progress on LF code signing infrastructure

* Larger / specialized GHA build instances

* [CMake version information](https://github.com/AcademySoftwareFoundation/aswf-docker/tree/master/ci-base) per CY.

* Updates to ASWF sample project
