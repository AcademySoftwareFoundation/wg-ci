---
title: 2024-01-03
parent: Meetings
---

# ASWF CI Working Group

Meeting:   03 January 2024

[https://zoom-lfx.platform.linuxfoundation.org/meeting/97730566101](https://zoom-lfx.platform.linuxfoundation.org/meeting/97730566101)

## Attendees

* Jean-Francois Panisset, VES Technology Committee
* Aloys Baillet, NVIDIA
* Jean-Christophe Morin, Res
* Andrew Grimberg, LF Release
* Ryan Bottriell, ILM
* Larry Gritz, Sony Imageworks

## Apologies

## New items

* aswf-docker updates
  * 2023.2 released
    * Regression with GLfw CMake not yet addressed for OpenVDB
      * Better understand CMake vs Conan, may be able to release 2023.3
  * 2024 progress
    * Current Status
      * Made it past building Qt6 and PySide 6 by disabling test suite
      * Thank you to Foundry Conan recipes, providing alternative to Conan Center Index
        * Useful to see how things can be done differently
        * Foundry-specific code reasonably straightforward to edit
        * Less "Conan-centric" than Conan Center Index recipes
      * Optix8 headers provided by NVIDIA
        * OSL doesn't support Optix8 yet
        * Larry: don't want to just download headers, more important to run the tests, I do it by now, but don't always remember to do these tests when merging someone else's changes. Want to see that come to an end!
      * Hopefully close to an initial release
    * Blockers
      * Having trouble getting CMake to pick up dependencies installed as Conan packages
        * When building containers, we force Conan packages to install in /usr/local (where they become just standard packages providing .cmake files), but when building Conan packages, we depend on Conan calling CMake to find installed Conan dependencies.
        * Lack of familiarity with CMake and especially CMake in Conan is causing problems
      * Mostly affects package test suites
      * Aloys offered to spend a bit of time "pair programming", JF to reach
* [How to handle the SonarCloud client](https://academysoftwarefdn.slack.com/archives/C0169RX7MMK/p1703612178914179)
  * Interceptor DSOs are not versioned, latest will be pulled when aswf-docker containers built
  * Scanner package is versioned and controlled by `ASWF_SONAR_VERSION` variable
  * Keep updating aswf-docker containers?
    * A significant amount of work
  * Provide a standard script / GHA snippet to update it?
    * Larry: changed it on OIIO, the GHA that Sonar suggests is very straightforward, I think the easiest thing to do is to stop putting it in containers, just add the 3 lines in their workflow. Unless you are worried about people using the containers outside of GHA, then maybe it's worth keeping it, but would they be using SonarCloud? We only expect SonarCloud to be used in context of GHA.
    * JC: I agree
    * Andrew: only exception if using a language not supported by GHA
    * JC: there are generic Actions, but I think we are fine. If someone is trying to use a language that's not supported, they will likely need to add custom code anyway. The Action just downloads from Sonar Cloud and unpacks it.
    * JF: for 2024 aswf-docker will remove SonarCloud and document breaking change
* OpenSFF Badging Requirements
  * TAC discussions on the requirement for Gold level for (new) projects to reach Accepted Stage
  * [Current project status](https://docs.google.com/spreadsheets/d/1n8xEdbJ77fVk5YxtuqjC7KZywi0W7ZfXlGf0YjVZI9Q/edit?usp=sharing)
  * Two PRs to look at:
    * [Adjust requirements for the OpenSSF Badge at the Adopted Stage](https://github.com/AcademySoftwareFoundation/tac/pull/556)
    * [Update the Best Practices Page with spots for detailed instructions on how to fulfill each requirement](https://github.com/AcademySoftwareFoundation/tac/pull/557)

## Follow Ups

* Updates on GHA paid resources
  * Larry: GPU runners. Wanted to see where we are / what's the status (was on vacation before). For my projects I never got the GPU runners working, even when they were running.
  * Andrew: access is restored, went GA beta towards end of year in 2023. They are now paid runners. We have full access, have the runner names in 2023-12-06 meeting notes.
  * Larry: will give it a try.
  * Andrew: Linux runner is the one published by NVIDIA. Larry: other people who want to use those NVIDIA containers probably have similar requirements. We may have better success getting action from NVIDIA.
  * JC: you can see the runners in your repository in Settings -> Actions, that should list the runners that can be used by the project.
  * Larry: main need would be for OSL.
  * JF: would want to transition OCIO to GHA.
  * Larry: how's the budget? Andrew: looking at Nov and Dec 2023, were just above $1K per month for usage. Haven't talked to John about approved budget, but have asked to double the budget, hopefully around $3K/month to play with (currently at $1500/month). Larry: I've added OIIO to do one test on the Mac ARM runners, want to make sure we didn't break the bank. Andrew: I get alerts early enough to make adjustments. Larry: let me know if it looks like any of my projects are hogging the budget, I can configure things about how often to run. For Mac I can keep it minimal, but for OSL I want GPU for every PR.
  * Larry: OCIO, OpenVDB and possibly MaterialX would also benefit.
  * JC: do all PRs need to run on GPU runners, or could it be based on tags? Larry: one of the tricks I played for OIIO to keep down Mac costs, I only do it on releases and PRs if the branch name contains a specific string. If you know you are working on something that might affect Mac ARM, you can force it to run with the branch name, otherwise it just runs once a week. For OSL I'm not sure people would have the judgment to avoid breaking, now that the GPU path is to pervasive, don't want to have to leave it to the submitter to determine whether to use it or not. JC: can use labels on PRs to trigger specific actions. Larry: can an admit add label after the fact? JC: yes. Larry: still a judgment call needed, but could be done if budget requires.
  * JC: could you look at what files are changed, and specific files that would trigger GPU builds? Larry: in some cases, maybe, but could be hard. One thing I can do is make the GPU run only run the GPU test suite, instead of the full test suite. The GPU runner can run just the GPU suite, to avoid burning up that time. Andrew: yes, that would be good. The Windows GPU runners are more expensive than the Mac ARM runners. Larry: don't think we use the Windows GPU runners? Andrew: we have defined Windows GPU runners, not sure if anyone is using them. Larry: my first stab at it will be a Linux / GPU test and assumes that if works under Linux, it will work under Windows. Unless we get Windows only breakage.
  * Andrew: I was asked about the Windows runners last month, that's why we defined them.
  * Larry: we try to avoid running all the possible combinations in CI, we don't "fill out the whole matrix".
  * JF: were we paying for AWS Codebuild via AWS credits? Andrew: AWS credits were provided based on estimated system usage and data transit. DPEL consumes the majority of these credits, so no longer using AWS CodeBuild won't make much of a difference. For OCIO already using CodeBuild, we should leave them there since it's "effectively free". In theory we could set up more projects on CodeBuild, we have a fair amount of AWS credits.
  * JF: could we look at [WarpBuild](https://www.warpbuild.com/)
  * Andrew: GitHub Actions Runner Controller [ARC](https://github.com/actions/actions-runner-controller): testing it out in AWS. If we are running into issues with finance, we could set up an ARC environment in AWS, give us a lot of what we get with GHA and premium runners, but would be consuming AWS credits, and have to manage systems. If we needed different GPUs as well, we could do it this way if AWS had GPUs we needed and which GitHUb doesn't provide.
  * Andrew: it's a K8S cluster, GHA talks to that controller. Define info in GitHub pointing to self hosted runners, can "scale to 0", but need the controller to always run and cluster defined (but could use cheap VMs).
  * JC: can also use cloud auto-scaler, but the master nodes have to run.
  * Andrew: my team is actively exploring this option for some other options, so if we need to go in that direction, it's not uncharted territory.
  * JC: Microsoft and Google are also premiere members, maybe we can get credits from them? So we could also run ARC in Azure or GCP.
  * Andrew: we are definitely tooled towards AWS or OpenStack than Azure / GCP. But we have some infrastructure in Azure and GCP.
* Andrew: GHA Team has asked if we wanted to have a meeting to ask questions. 30 minutes / 1 hour to go over upcoming GitHub functionality / issues we are running into.
  * No action so far (JF TODO), we should seize this opportunity
  * Andrew: on us to come up with an Agenda and proposed dates. JF: hopefully we can use one of the timeslots of this meeting. No further update.
  * JF: will bring up on Slack

## Tools and Links

* [WarpBuild](https://www.warpbuild.com/) Third party, cheaper GHA Runners
* [Slack discussion on conditional GH Actions](https://academysoftwarefdn.slack.com/archives/C0169RX7MMK/p1702497394491979)
* [Slack discussion on issues with automatic Slack release announcements](https://academysoftwarefdn.slack.com/archives/C0169RX7MMK/p1702710413394739)
* [sccache: Persistent S3 storage backend for ccache](https://github.com/mozilla/sccache)
  * Larry: OIIO and OSL will use ccache if detected and get a lot of mileage, will cache out the ccache cache between runs, so builds on the same branch will inherit the ccache. JF: where does it go? Larry: not clear, but there are limits in storage. If you go out to further storage, you may get more storage, but could lose on transfer time? JC: data locality is a concern, want the data to be as close as possible. It's probably Azure blobs, but sometimes you can get hangs downloading cache elements. JF: would S3 storage be more attractive if we had AWS runners? JC: we may have explicit control over caches with self hosted runners. Aloys: ccache caches can be multi-GBs, so having them close by is important, can be slower to download the cache than doing the build.
  * Ryan: we get good reuse with ccache, but there are tricks and topics to optimizing
  * Aloys: the ccache cache key can be clever and include several parameters. Larry: does it do fall back automatically? JC: you have to configure the hierarchy, with the first (or last) entry being the fallback. If you just specify a single key and it's not the one used by the main branch, it won't be used for newly created caches. Larry: do you know a project that "does it right"? I look at my projects and think I only specify a single key. I'd like to see an example that does it correctly. JC: the documentation for the cache GHA action probably has some of that info.
