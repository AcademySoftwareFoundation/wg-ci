---
title: 2023-07-19
parent: Meetings
---
# ASWF CI Working Group

Meeting:   19 July 2023 \

[https://zoom-lfx.platform.linuxfoundation.org/meeting/97730566101](https://zoom-lfx.platform.linuxfoundation.org/meeting/97730566101)

## Attendees

* Jean-Francois Panisset, VES Technology Committee
* Andrew Grimberg, LF Release Engineering
* Kerby Geffrard, OpenRV
* Larry Gritz, Sony Imageworks
* Mike Rocherfort, Skydance Animation
* Christina Tempelaar-Lietz, ILM
* Jeff Bradley, Dreamworks

## Apologies

*

## New items

* VFX Reference Platform User Group on Tuesday August 1st at 13:00 PDT
    * No registration needed, Zoom link will be sent to vfx-platform-discuss mailing list and likely other channels
    * Short 5 minute presentation of aswf-docker project, "an implementation of the VFX Reference Platform"
* MacStadium Intel MacMini renewal
    * We signed up for "1 free Intel Mac Mini for Open Source projects", but never really used it
    * Requires renewal / re-registration, should we bother? "We don't want to sysadmin our CI resources"?
    * Andrew: GitHub is going to be going beta in September for Apple Silicon runners, and we should be able to be in that beta. It is thanks our getting Apple and GitHub together. Larry: we haven't really set up, and it's hard to imagine projects having the spare cycles to set up on Mac Stadium.
    * JF: will just let it expire
* Upcoming ASWF Docker 2023.1 updates
    * NVIDIA Optix SDK includes
        * 7.0.0, 7.3.0, 7.4.0, 7.5.0, 7.6.0, 7.7.0
        * Installed in /usr/local/NVIDIA-OptiX-SDK-7.x.x/include
        * URLs provided over email, should be OK to bundle in containers.
        * Would be nice to have a cleaner way to download.
        * This is just the headers: will need -v volumes to allow the container to access the host-installed DSOs (in Optix 7, the DSOs are part of the NVIDIA driver)
        * JF: would need a "hello world" OptiX app to test running in GHA
        * Larry: Eric Enderton may be able to help / know who to talk to. JF: TODO: will reach out.
    * Updated 2023 packages
        * OpenEXR 3.1.9 (from 3.1.8)
        * OpenImageIO 2.4.13.0 (from 2.4.9.0)
        * OpenShadingLanguage 1.12.13.0 (from 1.12.10.0)
        * Alembic 1.8.5 (from 1.8.4)
        * Partio 1.17.1 (from 1.14.6)
        * Cmake 3.26.4 (from 3.25.2)
        * Pybind11 2.9.2 (from 2.8.1)
        * Python 3.10.11 (from 3.10.9)
    * Additional packages built as Conan-only
        * Boost
        * Clang
        * Cmake
        * Cppunit
        * Glew
        * Glfw
        * Log4cplus
        * pybind11
    * Default tagging for -clang15 images
        * Previous years used oldest clang version for default version tag, now using latest
        * ci-common:3 now points to :3-clang15
        * ci-{openvdb,osl,vfxall}:2023 now points to :2023-clang15
        * Fixed issue in 2023 images that failed to create a "default non-clang" version tag for clang built containers
    * Should we bump CUDA from 11.8 to something newer?
        * CUDA 12.x requires NVIDIA driver 525.60.13, GitHub beta GPU runners only have 470.82.01 (can we ask for newer?)
        * Or is sticking with 11.8 good enough for 2023?
        * Andrew: I asked about the GPU driver version in GitHub meeting. They don't use the images they use, they use images from Azure, don't plan to update what Azure is providing. So we would need to pressure Azure to get an image updated. TODO: JF to check on latest Azure GRID driver
        * Mike: if GitHub is using Azure instances, if someone is an Azure client for Render, they could also open a ticket. JF: the GHA GPU image is Ubuntu based.
    * VirtualGL ?
        * Mike: how about xvfb? JF: not sure that implements DRM? Mike: I've used on HPC cluster for systems without an X server, but not sure those were using CPU or GPU accelerated rendering.
        * JF to follow up
    * Struggling with large increase in Docker build context size for local build with Conan build cache (clang, Qt, PySide...), build time vs runtime caching, BuildKit...
        * Hoping to have a solution so 2023.1 can be pushed out
        * Mike: if that becomes a problem, if you want could lend a hand to building RPMs of these packages.
* Impact of recent Red Hat announcement on third party RHEL rebuilds
    * Rocky Linux (used for aswf-docker 2023) still updated for now, but future uncertain
    * Alma Linux no longer "bug for bug compatible"
    * RHEL UBI doesn't currently have all the packages we use
        * Can we trim some packages? Will users be affected if they can no longer compile ffmpeg (say) in vfx-all?
        * What's the scope of what should be included as -dev packages?
        * Mike: UBI package set is a stripped down variant of what's in RHEL proper, not the full RPM set. Some of the components we are pulling directly from Rocky, there is a repository called "devel" which isn't a RHEL repository, it's a set of RPMs coming from the distribution build root so that people could use in EPEL for instance. Some of the packages we are using don't exist in RHEL. If we can comb those out, it would make things easier.
        * Mike: with this change from Red Hat, there's an issue around UBI, since it is known that rebuilds will use SRPMs available through UBI, it will make it a harder sell to add more packages to UBI. It hinders trying to use UBI. I would be very tentative to take Rocky's word given their statement as a "loophole" for access to SRPMs. From Alma perspective, it's still possible to reproduce a "1 to 1" rebuild, but requires more work.
        * Mike: still planning to see what I can come up with UBI, will talk with Bob Davis at Red Hat, see what we can get added, hopefully they will accept them and add them in. Also UBI only has a "latest" version, so when gcc 13 comes out, gcc 12 will be removed. Maybe there's a way to add those back in, or set up a side mirror / repo so we can keep that history going. Non trivial amount of work needed to make UBI a valid base to build on. Up to RedHat whether we can use this, and images need to be redistributable. Can't redistribute a RHEL based image.
        * JF: install-yumpackges.sh script isn't everything: ASWF projets may be adding their own -devel packages. Mike: yup, moving from common to base to packages containers so we can create final package. Wouldn't plan on 2023 images based on UBI, but will def work on it.
    * May be interesting to experiment with building aswf-docker on alternative platform (say Ubuntu)
* Open discussion: GUI app testing?
    * In scope?
        * Andrew: I am positive there is a positive there is a open source GUI testing toolset, but don't know details. There are a few projects associated with releng, but don't have actual GUI, they are all web based, so use frameworks like selenium or playwright for web based applications. Nothing for X11 / OpenGL apps.
        * Jeff: we've experimented with [SikuliX](http://sikulix.com/) , take snapshots of the screen, there's flexibility so it doesn't require pixel perfect matching. Not sure if it's going to gain momentum, there are concerns that this is less flexible than unit tests. But then unit tests can change as well. There's also [Squish](https://www.qt.io/product/quality-assurance/squish) from Qt. JF: no free for open source. Kirby: but we can ask, you never know.
        * Jeff: we've used SikuliX for performance testing that encapsulates user experience, all the way through NFS, services... So it can measure what the artist experiences. Could arrange a short demo at future CI meeting. Can't just run it on local machine with your own preferences... People move things around, establishing a standard machine with clean user preferences has been crucial.
* Commercial DCC integration
    * Move Houdini download project somewhere else? [https://github.com/tykeal/sidefx-web-cli](https://github.com/tykeal/sidefx-web-cli)
        * Andrew: would have no problem moving it somewhere else, haven't done anything with it since. It never moved into ASWF at the time due to repository management, hadn't followed processes to become a repository inside ASWF. But we have created repos outside the standard workflow (like ci-wg). I can migrate it. JF: not sure if OpenVDB uses it, or still uses screen scraping.
    * Would OpenFX want a Nuke / Resolve / Natron download for its projects?
        * JF to reach out to OpenFX
* Candidate VFX centric libraries for inclusion in vfx-all?
    * For instance: [OpenImageDenoise](https://www.openimagedenoise.org)

## Follow Ups

* Create a RSS channel in ASWF Slack (JF TODO, not done yet)
    * Larry: can watch a GitHub project as well, so can set that up to only get notifications about releases (assuming project owners make tagged releases).
* GitHub Updates (Andrew)
    * See update about Apple Silicon availability in beta in September
    * GPU driver coming from Azure supplied image
    * Some discussion related to GitHub Enterprise Account, different from GitHub Enterprise Organization (which ASWF has), which gives us higher end features, and parallel job build, less limitations than standard GitHub account. LF is in the middle of signing deal with GitHub to request GitHub Enterprise Accounts, easier to manage organizations and billing. Only affects ASWF if we end up needing multiple organizations. That is going to be something we can "ask for free", but will need to go through a specific process for that. There are a couple of organizations that haven't moved to the ASWF org (OTIO? Need multiple repos / manageability). So in near-future, may be able to consolidate billing for those additional orgs.
    * For CoPilot: from LF point of view, going through legal review, whatever a project decides to do with CoPilot, LF has no legal guidance. But for LF properties, not allowed to use CoPilot, the legal / license status hasn't been established yet. But GitHub will announce new features of CoPilot which will allow organizations to specify whether CoPilot can be used, and some controls on it. Larry: will there be training on only compatible licenses? Andrew: yes, that's what's supposed to be released. Larry: having played around with it, I find it easy to distinguish between 2 use cases. One is a "fancy auto complete", vs the uses where it writes a function for you, and you don't know where it comes from. Those are not hard to separate, and I've mostly used the first type. Suspicious when it gives you a big block of code.
    * JF: what's the pricing model for CoPilot? Andrew: you can get CoPilot for free as an open source contributor for specific uses. But if you are trying to work directly inside a repo for an org that has disabled it, it won't be available. JF: what if you are working in VS Code? Larry: it's a VS Code plugin. Andrew: not sure if it knows if you are working in a local repo clone. But if someone has been flagged as an open source developed, it will probably be available through your standard GitHub account. So permission only affects if using CodeSpaces. Larry: don't know what the VS Code plugin would do on a fork of an org that disables CoPilot. It's a lot like having an intern: sometimes it's a "magical auto complete", sometimes it does nonsensical / wrong things, you have to double check its work. It can save a lot of typing!
* Need to produce some kind of deliverable from our CII badging discussions
    * [https://github.com/AcademySoftwareFoundation/tac/pull/376](https://github.com/AcademySoftwareFoundation/tac/pull/376)

## Tools and Links

* Does anyone still use Valgrind? If not, why not / any alternatives?
    * [https://valgrind.org/docs/manual/index.html](https://valgrind.org/docs/manual/index.html)
    * MTuner (alternative to Valgrind):
        * [MTuner](https://milostosic.github.io/MTuner/overview/why/)
    * Valgrind Massif (heap profiler):
        * [Valgrind Massif](https://valgrind.org/docs/manual/ms-manual.html)
    * Heaptrack
        * [Heaptrack](https://invent.kde.org/sdk/heaptrack)
* Digger: run Terraform from CI:
    * [Digger](https://github.com/diggerhq/digger)
