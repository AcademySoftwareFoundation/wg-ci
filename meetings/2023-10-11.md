---
title: 2023-10-11
parent: Meetings
---

# ASWF CI Working Group

Meeting:   11 October 2023

[https://zoom-lfx.platform.linuxfoundation.org/meeting/97730566101](https://zoom-lfx.platform.linuxfoundation.org/meeting/97730566101)

## Attendees

* Jean-Francois Panisset, VES Technology Committee
* Kerby Geffrard, OpenRV
* Larry Gritz, Sony Imageworks
* Andrew Grimberg, LF Release Engineering
* Mike Rochefort, Skydance Animation
* Christina Tempelaar-Lietz, ILM

## Apologies

## New items

* ASWF Dev Days start tomorrow
  * Lots of stated interest and inquiries
  * Keep an eye out for requests for help for projects you are familiar with
  * Larry: co-chairing with Carol, when we started it would be great to get 10 or 20 people, we ended up with 75 sign ups, there will be attrition, some people won't be able to do it, or get halfway through the day and realize they are missing prerequisites, but interest is well above expectations. The spreadsheet of all the people who responded on the Google form, there are 3 "classes" of people: some are the people who are truly new, will be interesting to see how many contributions we get to them. Other class is experienced software engineers but new to Open Source, ILM did a great job of roping in people from their software department. Then there's a third class who are experienced open source contributors and working on projects, but choosing to work on a different project. Hopefully projects will get someone who 'sticks around'.
  * Kerby: OpenRV is part of Dev Days, part of a project with Walt Disney about some review protocols. Will have 4 people available to help as part of their work day.
* #release-announcements Slack channel
  * Updated to run explicitly off GitHub Actions instead of RSS feed, can be customized for each project: [release-notice.yml](https://github.com/AcademySoftwareFoundation/aswf-sample-project/blob/main/.github/workflows/release-notice.yml)
  * Mike: there is a an open source project called [Anitya](https://release-monitoring.org/) which can be used to monitor package releases. Projects such as Debian, Fedora, CentOS all use this to monitor packages.
  * JF: would give you better control than dependabot? Mike: focuses on tags, so every time there's a tagged release, will analyze it based on semantic versioning, early access vs GA tags, keeps a running history of every version it has gathered. Will add a link to the Fedora instance used at scale. JF: could be very interesting for projects with many dependencies.
  * Andrew: Looks like it publishes to RabbitMQ via Fedora Messaging.
  * [anitya source repo](https://github.com/fedora-infra/anitya)
  * Does involve some self hosting, but could be worth trying out. Of course then there's a cost / maintenance overhead.
  * Andrew: if it's useful enough for enough projects, LF IT might take it on as a service provided to projects, not just ASWF.
* VFX Platform 2024 containers
  * No real progress, but getting close to local Conan cache solution
  * Will likely release 2023.2 update
    * OpenEXR and OpenVDB both want pybind11 in their ci- containers
    * Fix issue with GLfw cmake setup for OpenVDB in 2023.1 (transition to Conan)
    * Question for Aloys: what is the ci-base image for? An image with all the tools but not specific to a project?
    * Your wishlist goes here?
* Opt-in / plugin library architecture for ORI
  * JF: example of projects using dlopen?
  * Mike: being done with ffmpeg, module loading being re-architected in ffmpeg (x264. x265) so they can be loaded at built time. So it's been going on for past two years in context of Fedora, being upstreamed into Fedora. Fedora doesn't ship those codecs, RPM Fusion repo has a version of ffmpeg which includes the links to those packages. So these dependencies don't need to be there at runtime for ffmpeg to run.
  * Neil Gompa is part of that initiative and could be contacted.
  * JF: had been thinking of leveraging OpenFX plugin architecture, but better to leverage existing work.
  * Any other examples we can think of?
  * Mike: Would xStudio benefit from that? They seem to have a bundled ffmpeg package that may or may not enable specific codecs.
* AWS resources for ASWF projects
  * What do we currently use
    * S3 buckets for DPEL (get credits from AWS)
    * CodeBuild for OCIO GPU CI (get credits from AWS)
    * JIRA instance, but unused, server variant
      * Separate than LF RelEng helpdesk for instance
      * JF: is there a cost associated? Andrew: between Jira and Confluence, looking at around $250/month, don't get AWS credits for those (JIRA / Confluence licenses are free for open source in our case)
      * Once Server fully EOLs, we have to move to datacenter (we could get open source licensing for that), or transition to open source licensing for cloud, license itself would be free.
      * Mike: does open source program provide unlimited users? Andrew: yes, for datacenter, but not for cloud, they can't have instances larger than a certain number of users. That's an issue for Jenkins project for instance.
      * Mike: limit for cloud would be above for LF, or just ASWF? Andrew: we are talking to them about siloed instances, not LF-wide. Number of seats consumed by ASWF would be separate from LF.
      * Andrew: looks like no one has used the JIRA instance since August 2022.
      * Mike: not sure how useful JIRA would be? Andrew: JIRA was initial request when ASWF was started, OpenVDB has used it, that's it. DPEL, OCIO and OpenEXR exist in the system but haven't been used.
      * JF: action item to ask if it was a problem to get rid of it.
    * Confluence instance (lightly used), server variant
      * JF: more value for ASWF projects in Confluence than JIRA? Few projects can devote project management resources to really leverage JIRA.
      * Mike: documentation platform would provide the most value. If you want to have ticketing as well, Youtrack could make more sense, but not sure anyone is interested in getting issue tracking off GitHub. Andrew: everyone is using GitHub issues.
    * Internal push to move over to cloud, lose some capabilities, but wouldn't affect ASWF
    * Atlassian will have to support the datacenter version for a long time, we could in theory transition to those versions
    * Artifactory is being provided by JFrog (cloud instance)
  * What is the approval process
    * John Mertic is the project manager for ASWF, so approves requests from LF RelEng. Can be authorized through TSC / TAC / Board of Directors. Board authorizes the budget, John holds the purse strings to the budget, TSC can request resources from John, and he can approve if it fits.
  * How are they deployed and managed
    * Typically through RelEng / service desk
  * Kerby: ORI needed a RabbitMQ server for Dev Days, will use an Autodesk AWS account. Andrew: going forward best way to do this would be to create a resource. John would authorize it.

## Follow Ups

* Restoring access to GHA GPU runners
  * Andrew: apparently the issue that got opened with GitHub helpdesk was closed, need to get it reopened.
  * Talking with another project, have been made aware of a project out of GitHub for self hosted runners that scales automatically, runs on top of Kubernetes, including scale to zero. We could effectively set this up inside AWS and consume some of the AWS credits. Also our OpenStack provider has provided a way to do this slightly more natively on OpenStack, would give access to ARM and GPU builders. If we go with our OpenStack provider, we don't have any credits there.
  * Mike: which group is using OpenStack? Andrew: all our Jenkins based projects are on top of OpenStack. AWS and Jenkins don't collaborate very well, Jenkins and OpenStack work well together. OpenStack plugin for Jenkins is the best dynamic build plugin I've dealt with. Haven't used the Kubernetes plugin, but have to have Jenkins inside Kubernetes. It's not JenkinsX, but it is container only, so can't do a "bare metal / VM" build. We don't run JenkinsX anywhere. Mike: before we were using GHA runners on Azure, if we can self host, we could have a bare metal host. Andrew: we've discussed this before, but with self hosting, the static setup of systems that are always running, that cost adds up a lot if it's not used 100% of the time. So we usually look for solutions that "scale to zero". We get this out of OpenStack, as well as [ARC](https://github.com/actions/actions-runner-controller) (Actions Runner Controller).
  * Mike: k8s not everyone's cup of tea, I've used it without issues, between that and OpenStack, don't know of a lot of platforms that have good auto scaling infrastructure. Also get kubevirt for VM builds.
  * Andrew: investigating ARC for a couple of other projects.
  * Mike: I would only recommend using k8s if you are using a managed service, otherwise will need dedicated people to manage it.
  * Andrew: looking at setting up ARC in AWS on top of EKS or something like that, or inside Azure which also has managed k8s (our OpenStack provider also has that). Have already asked our OpenStack provider for some features, they are developing their own infrastructure.
  * Mike: can you also do on-demand scaling? Andrew: yes, ARC can scale to zero, also native OpenStack one can scale to zero. But sometimes we may want minimum number of runners during certain times, than scale to zero at other times. So may want to have a few runners running at all time for quick turnaround. Andrew: some of our projects are very busy during some times, so want to give them runners with low latency at those times. But don't want to waste money on idle systems.
  * JF: maybe we could have more cost effective options than GHA runners? Andrew: we may have more control over exact details of what we run on. AWS does offer ARM, and our OpenStack provider as well, so we have a couple of ARM options, which we don't get at all from GitHub.
* [Apple Silicon GHA runners in early access](https://github.blog/2023-10-02-introducing-the-new-apple-silicon-powered-m1-macos-larger-runner-for-github-actions/)
  * Cost will be an issue, projects will need to make judicious use of these builds
  * JF: will they incur cost during beta / early access? Andrew: if they've published cost, we will have costs, and will come out of our $1500/month budget for runners. We're currently averaging $1200 / month.
  * Larry: the number that was published seems had to afford. Basically $1 a CI run, $0.16/minute, the one test I want to run for OIIO takes 10 minutes, if people submit several PRs a day, it will add up. But don't want to lose ability to do this, so don't make it run on every PR, but will run on releases, and any PR where the branch has a special name, i.e. a branch where someone is specifically trying to fix an issue on ARM / Apple Silicon. What I want to make sure is we don't put out releases that are broken on that platform. But 20x more expensive than the Linux runners.
  * Larry: back of envelope calculation, got close to yearly cost close to buying a new Mac, if across all of the projects we could have a machine we own, but wouldn't want to support it. But might make sense across all the projects.
  * Mike: if Linux Foundation has colo site? Andrew: we don't run any. All projects that run hardware take care of colo themselves.
  * Larry: hopefully over time more developers will be working on Apple Silicon, so will have better coverage at the developer level. Mike: is there an option of enabling local builds from CI / run a GHA runner on their local system? Andrew: it's technically possible, not sure if we want to say yes to that. We run into potential issues around security when we say yes to something like that. Mike: yes, that's the big issues. Would need to be able to limit running only PRs from specific users on those systems. Andrew: not that I'm aware of this. I have a developer who has been trying out [nektos/act](https://github.com/nektos/act). Lets you run GHA locally in a Docker container. Doesn't work for everything, but works for some things.
  * Larry: there's a cap on monthly spend, can there be caps on individual projects? Andrew: unfortunately no, I wish it was possible. Larry: I don't want to use the whole project with my own project. Andrew: let me check, but no option to set billing limits at project level, only at organization level. Can bring that back to GitHub as a request.
* CI WG Project Requirements Survey
  * No progress yet
* Breakdown of outstanding CII badge issues for OpenEXR and MaterialX
  * [ASWF Project badge analysis](https://docs.google.com/spreadsheets/d/1bEacUNFizeT8QtfsvqiRNNgvty8_tweHjassHko6OhQ/edit#gid=361431482)
* PyPI organization level accounts

## Tools and Links

* [OSL code to build with Intel compilers](https://github.com/AcademySoftwareFoundation/OpenShadingLanguage/blob/00ea8a80fd6ea3064a7f5fa769fb02b526a7ac45/src/build-scripts/gh-installdeps.bash#L43)
* [OpenSSF Best Practices: Source Code Management Platform Configuration](https://best.openssf.org/SCM-BestPractices/)
* [LLVM 17.0.1 release](https://discourse.llvm.org/t/llvm-17-0-1-released/73549)
* [macOS Containers](https://macoscontainers.org/)
  * Native macOS containers on macOS
  * Currently requires disabling SIP
* [gcc toolset 13](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8-beta/html/8.9_release_notes/new-features)
* [Python 3.12.0 from a supply chain perspective](https://sethmlarson.dev/security-developer-in-residence-weekly-report-13)
* [Docker Scout Software Supply Chain Scanning / SBOM](https://docs.docker.com/scout/?utm_campaign=2023-10-10-launch-scout-ga)
  * How does this compare to [Snyk](https://snyk.io/)?
