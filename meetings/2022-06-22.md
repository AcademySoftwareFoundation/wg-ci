---
title: 2022-06-22
parent: Meetings
---
# ASWF CI Working Group

Meeting:   22 June 2022

[https://zoom.us/j/757849640?pwd=QzE1K2hrL2FHSFhKK3h5Z3BWTFJsZz09](https://zoom.us/j/757849640?pwd=QzE1K2hrL2FHSFhKK3h5Z3BWTFJsZz09)

## Attendees
* Jean-Francois Panisset (VES Technology Committee)
* Andrew Grimberg (LF RelEng)
* Aloys Baillet (NVIDIA)
* Jean-Christophe Morin
* Larry Gritz (Sony Imageworks)
* Esteban Papp, AWS, Creative Tools
* Jeff Bradley (DreamWorks)

## Apologies

## New items

* Welcome to Esteban from AWS
    * They are using aswf-docker containers
* Follow up on aswf-docker containers and VFX Platform 2023
    * What to use as a base: Rocky 8 (8.6), [RedHat Universal Base Image](https://www.redhat.com/en/blog/introducing-red-hat-universal-base-image) 8 (ubi8)?
        * Not widely known, people may be afraid to use them for licensing reasons?
        * Also UBI 7 containers available
        * Andrew: not aware of any projects using those containers, there may be LF projects using them, but don't know of any. RelEng isn't supporting anyone using them.
    * What CUDA version, 11.7?
        * Larry: OSL uses CUDA directly, what would be a desirable version of CUDA for 2023? Aswf-docker gets to "set the standard" for CUDA. Imageworks using CUDA 11.3 right now, also Optix 7.3, may push to a new one.
        * Sometimes newer CUDA releases sunset older GPU architectures.
        * Outstanding PR for AMD RoCM support should be looked at.
    * Should we roll our own base images instead of using the ones [provided by NVIDIA](https://gitlab.com/nvidia/container-images/cuda/-/blob/master/doc/supported-tags.md#rockylinux8) which could allow using Alma Linux, and/or building RHEL 9 or derived runtime containers (CY2023 more or less mandates building in an "8" environment, but could be interesting to prototype a "9" build / runtime environment)
        * Do we need other combos? Too many?
    * Anything we can do as a group to help start this process?
* Projects which need better builders
    * OpenVDB severely limited by free builders, suggesting they use AWS CodeBuilder
    * Estimate is $200-$300/month in usage
    * Looking for numbers on what OCIO spends on GPU-accelerated builds
    * Anyone know what the process was to get this expense approved? (or in general any infrastructure-related expenses)
    * Any other projects could benefit from larger builders?
    * Should we consider a shared, large full time machine for ASWF builds (CodeBuild can quickly reach the cost of a dedicated system). What do other LF-affiliated foundations do?
    * Is AWS comping us? Andrew: no, we went through trying to get credits, and it didn't pan out, so right now we are paying for this. Some of our (LF) projects manage to get them, some don't. Larry: we should talk to Sean Looper from AWS.
    * Andrew: looking at ASWF AWS expenditure this year, currently at $6.5K, is it just CodeBuild, or is there other stuff in there. Larry: OSL has been lazy about setting up GPU builds, but really need to do it. So that doesn't count OSL needs, should be at least as much as OCIO. Could throttle it, but GPU rendering is becoming more important to OSL. Andrew: May jumped by $1,100, not sure what happened.
    * Andrew: have detailed tracking. Have projects labeled managed and unmanaged.
    * Esteban can ping Sean Looper, keep Andrew in the loop. He's not in the group that assigns credits.
    * Andrew: RelEng doesn't manage any full time runners, have historically pushed for any cloud type builds to go zero, if there's no builds happening, there are no runners. Preferred method since it's the most cost effective, issue is started up cost, and how it connects to CI system. For GHA would need either CodeBuild triggered from GHA, or we have a dedicated set of systems running all the time. One of LF projects wants to do this in GitLab, they have a tutorial how to have dynamic runners in GitLab, so you can use stuff outside their pool that don't consume your free minutes. Andrew: how big a system do you need, and do you need GPUs, that jumps the cost a lot. Esteban: have you done an analysis of a big projects? Andrew: for ASWF, the only project doing anything in ASWF is OCIO, they only run the test suite. All the other projects that RelEng supports don't use AWS, except one. These all follow the same pattern with dynamically launched systems, using Jenkins.
    * Esteban: use to work on ODE3 (was a game engine, now a 3D engine), ran Jenkins, backend infrastructure in AWS, broke down the builds, pick instances based on build requirements. Andrew: RelEng Jenkins does this as well. But the only project we have this is OCIO only needs GPU.
    * OCIO only runs the test suite, not the builds, possibly once a day.
    * Dedicated full time system would be treated as a "pet", would incur management code. ASWF itself for the service tear it gets from LF, it would be covered. Only passthru cost of full time instance would be an issue.
    * What's the process for approving new expenses? John Mertic is project manager and holds the purse strings. He would know our current budget for cloud builds. There is a budget, we've spun up and shut down various components, DPEL has costs for S3 buckets for instance. JF to reach out to John, and see if we need to reach out to Board.
    * Aloys: Docker infrastructure could also use larger builds, can't really build Qt, becoming a problem, it's not a blocker, but it's annoying. In this case it's number of cores and time (times out after 6 hours). Andrew: this can be fixed with a bigger system, or do a custom runner to get more time. GHA custom runners could have no time limits.
    * Aloys: it's very occasional, only rebuild these packages 2-3 times per year.
* PyPI documentation and organization
    * OpenVDB, Rez want to publish to PyPI, OTIO already does
    * No support (yet) for [organization-level accounts in PyPI](https://github.com/pypa/warehouse/issues/201)
    * Should every project do its own thing? Does LF RelEng have credentials for those accounts?
        * RelEng doesn't have OTIO credentials
        * JC: there are 4 people who can publish artifacts to PyPI, so there are 3 people as backups.
        * Andrew: if the community has to have credentials, recommend minimum of 3 people. Preferable if they are not from the same company.
        * JC: 3 at Pixar, one at Netflix.
        * Andrew: RelEng could hold the keys, we have one for our own packages that we publish to PyPI. Publish from a role based account from CI systems.
        * John has mentioned a 1Password infrastructure? Andrew: 1Password offers a free enterprise edition to Open Source organizations. ASWF has gotten it, but not managed by LF RelEng. So that's one way to store shared credentials.
    * What would be the "right" way to organize accounts and credentials
        * Andrew: if we want to publish under a ASWF "banner", would need a single account to do that. For projects that already exist on PyPI... PyPI is not namespaced, it's a flat namespace, unlike Docker Hub or GitHub. PyPI is all flat namespace with a unique name. All packages from ASWF would go under their own name, but it's who has access to it.
        * JC: you have to add the "common" user as the maintainer of the package. Andrew: RelEng has role based account which has owner rights. JC: other solution is to wait for organization based accounts to show up at end of year (non verified information). You can transfer the ownership of a package to a different user. With the organization feature, there will be a way to transfer a project from an individual account to an organization account. Andrew: recommend that individuals don't push packages directly, but instead do it via CI. GitHub maintainer would have the ability to trigger workflow that publishes. Our PyPI packages are published based on tagging of a repo which triggers a build. JC: OTIO works this way, not based on tagging, publishes to PyPI when a release is created.
        * Andrew: RelEng uses Gerrit to do this, gives fine grain control.
        * Wiki open on wg-ci repo, need to document here
        * Andrew: GitHub allows a user account to become a organization account, will PyPI support that?
        * Should we try to grab an ASWF PyPI account?JC: there will be a form to apply an organization, will be a paid for feature. Someone will approve or deny to create the organization. Give the name, describe what it does...
        * Andrew: roadmap : [PyPI Organization Accounts Discussion](https://discuss.python.org/t/pypi-organization-account-roadmaps/14699)
        * JC: [GitHub Project for PyPI organization accounts](https://github.com/orgs/pypa/projects/5)
        * Andrew will try to grab a "aswf"? JC: may not be ready for it.
        * Larry: there's a set of PyPI OpenEXR bindings made by someone not part of the project, they may not be up to date.
        * JC: we want an official "checkmark"
        * Andrew: we could consider a namespacing of packaging, but would need to make sure all the packages to migrate to a single namespace. Do we want to do that? Not sure the amount of coordination this would be needed. JC: can create lots of different problems in code bases.
* Docker Hub credentials and organization
    * aswf and aswftesting accounts so far mainly used for aswf-docker images, what about other projects, how best to organize?
    * Should Docker Hub credentials / secrets be moved up to organization level so they are accessible by every project?
    * Docker Hub credentials are at the ASWF Org level, so every project should have access. JC: usage of Rez images are not made to be used be used by others, they are just an intermediate build artifact. Should we have an "aswfinternal" namespace? Andrew: for containers that are just needed for internal builds, should use the GitHub internal registry? JC: Alan wanted to use Docker Hub. Andrew: it really should be in something not publicly available without jumping through hoops. The GitHub registry may not be public by default, which is where intermediate artifacts should live. Should also in theory be "faster"? Andrew: run Nexis3 for Jenkins builds, 3 separate registries, with "snapshot" for non-release artifacts. Only repo that goes all the way to Docker Hub is the release repository.
    * Need to use a different registry / set of macros in GHA. JC: should be possible to use the GitHub registry.
    * JF: may want to edit the Docker Hub home page.
    * JC: some projects public to both GitHub Registry and Docker Hub. Helps with discoverability.
* Dependency scanning using tools like snyk:
    * [PR time (static) dependency review](https://docs.github.com/en/code-security/supply-chain-security/understanding-your-software-supply-chain/about-dependency-review)
    * [Build time dependency review](https://github.blog/2022-06-17-creating-comprehensive-dependency-graph-build-time-detection/)
    * Would aswf-docker project be a good place to demonstrate this, or somewhere else?
* macOS runner
    * We still have Intel Mac Mini on Mac Stadium available, interest in a full time runner?
    * If this works out, could we justify renting a full time M1 Mac Mini ($132-$152 per month for 8/16GB), or keep waiting for AWS offering?
    * [Blog post on setting up a M1 GHA runner using native virtualization support in macOS](https://mirkogalimberti.com/post/2/github-actions-self-hosted-apple-silicon-m1-runner-howto)
* Bonus topic: should we try to do anything about libpng?
    * Settle on a viable, better maintained fork (is there such a thing?)
    * Encourage ASWF projects to move to a different implementation (any ASWF projects use libpng directly?)
    * Add native libpng support to OIIO

## Tools

* Prototype aggregate RSS feed for ASWF project releases, useful? [https://zapier.com/engine/rss/12773997/aswf-release-feed](https://zapier.com/engine/rss/12773997/aswf-release-feed) Where would such a tool best run otherwise?
* [Chalet](https://www.chalet-work.space/): A cross-platform project format & build tool for C/C++ focused on usability and interoperability.
* From the Google V8 blog: [Retrofitting Temporal Memory Safety on C++](https://v8.dev/blog/retrofitting-temporal-memory-safety-on-c++)
* [Shottr](https://shottr.cc/): a macOS utility for capturing, managing and annotating screenshots, useful when writing documentation

## Follow Ups

* Overlap with USD WG (every other week, alternates with TAC)
    * No update yet, unclear what's the solution
* Updates on GHA custom / for pay instances (Andrew)
